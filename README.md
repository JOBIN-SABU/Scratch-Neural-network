# Neural Network from Scratch ðŸ§ 

A minimal implementation of a single-layer neural network using only **Python** and **NumPy** â€” no external ML libraries.
This project demonstrates the **core mechanics of neural networks**, including forward propagation, activation functions, loss calculation, and gradient descent.

---

## âœ¨ Features

* Implementation of a custom `Neuron` class.
* Forward propagation with **Sigmoid** activation.
* Loss function: **Mean Squared Error (MSE)**.
* Weight and bias updates using **Gradient Descent**.
* Trained on a small dataset (e.g., Iris dataset).
* Modular design with getter/setter methods for weights and bias.

---



## ðŸ“Š Example Output

* Training loss decreases with iterations.
* Accuracy improves over epochs.
* Final trained neuron classifies Iris dataset samples better than random.

---

## ðŸ“– Concepts Covered

* Forward propagation
* Sigmoid activation function
* Gradient descent update rule
* MSE loss function
* Basics of backpropagation (derivative of loss w.r.t weights & bias)

---

## ðŸ”® Future Improvements

* Add support for multiple layers (MLP).
* Implement more activation functions (ReLU, Tanh).
* Try classification loss (Cross-Entropy).
* Experiment with other datasets (MNIST, Wine dataset).

---

## ðŸ“œ License

MIT License â€“ feel free to use and modify.

---
